{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9c1a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm \n",
    "from colorama import Style,Fore\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8727f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openJson(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def saveJson(path,data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "       json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "       print(Style.BRIGHT+Fore.GREEN+'\\n json saved'+Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c29a7",
   "metadata": {},
   "source": [
    "# Short Bio probleme resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "797f4b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1836"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channelsF2 = openJson(\"../jsons/channelsF2.json\")\n",
    "len(channelsF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f475b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plages = {\n",
    "    '0-10': 0,\n",
    "    '10-50': 0,\n",
    "    '50-100': 0,\n",
    "    '>100': 0\n",
    "}\n",
    "\n",
    "for item in channelsF2:\n",
    "    bio = item.get('bio', '').strip()\n",
    "    nb_mots = len(bio.split())\n",
    "    \n",
    "    if nb_mots <= 10:\n",
    "        plages['0-10'] += 1\n",
    "    elif nb_mots <= 50:\n",
    "        plages['10-50'] += 1\n",
    "    elif nb_mots <= 100:\n",
    "        plages['50-100'] += 1\n",
    "    else:\n",
    "        plages['>100'] += 1\n",
    "\n",
    "labels = list(plages.keys())\n",
    "counts = list(plages.values())\n",
    "total = sum(counts)\n",
    "percentages = [count / total * 100 for count in counts]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(labels, counts, color='skyblue')\n",
    "\n",
    "for bar, pct in zip(bars, percentages):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, height, f'{pct:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('Plage de nombre de mots dans le bio')\n",
    "plt.ylabel('Nombre de chaînes')\n",
    "plt.title('Distribution des chaînes selon la longueur du bio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5051f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapetube import get_channel\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "def getContext(channel_id):\n",
    "    api_key = os.getenv(\"YOUTUBE_API_KEY2\")\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    video_ids = []\n",
    "    for video in get_channel(channel_id, limit=3, sort_by='newest', content_type='videos'):\n",
    "        video_ids.append(video['videoId'])\n",
    "\n",
    "    if not video_ids:\n",
    "        return \"No videos found.\"\n",
    "\n",
    "    request = youtube.videos().list(part='snippet', id=','.join(video_ids))\n",
    "    response = request.execute()\n",
    "\n",
    "    combined = \"\"\n",
    "    for item in response['items']:\n",
    "        title = item['snippet'].get('title', 'No title')\n",
    "        description = item['snippet'].get('description', 'No description')\n",
    "        tags = ', '.join(item['snippet'].get('tags', [])) if 'tags' in item['snippet'] else 'No tags'\n",
    "\n",
    "        combined += f\"Title: {title}\\nDescription: {description}\\nTags: {tags}\\n\\n\"\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "for item in tqdm(channelsF2):\n",
    "    bio = item.get('bio', '').strip()\n",
    "    nb_mots = len(bio.split())\n",
    "    \n",
    "    if nb_mots <= 100 :\n",
    "        context = getContext(item['id_chaine'])\n",
    "        item['context'] = context\n",
    "        \n",
    "    temp+=1\n",
    "    if temp >= 100:\n",
    "        saveJson(\"../jsons/channelsF2.json\",channelsF2)\n",
    "        temp = 0\n",
    "   \n",
    "saveJson(\"../jsons/channelsF2.json\",channelsF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d5611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d50fdc1",
   "metadata": {},
   "source": [
    "# 1.Echantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949facc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "echantillon = random.sample(channelsF2, 200)\n",
    "\n",
    "for item in echantillon:\n",
    "    item['link']=f\"https://www.youtube.com/channel/{item['id_chaine']}\"\n",
    "    item['pertinente']=\"\"\n",
    "    \n",
    "saveJson(\"../jsons/echantillon.json\",echantillon)\n",
    "print(len(echantillon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c1168",
   "metadata": {},
   "source": [
    "# 2.Manuel Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081a8395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echantillon = openJson(\"../jsons/echantillon.json\")\n",
    "len(echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa6a0bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non → 166 chaines\n",
      "oui → 34 chaines\n"
     ]
    }
   ],
   "source": [
    "valeurs = [item.get(\"pertinente\") for item in echantillon]\n",
    "compteur = Counter(valeurs)\n",
    "\n",
    "for valeur, nb in compteur.items():\n",
    "    print(f\"{valeur} → {nb} chaines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512537a2",
   "metadata": {},
   "source": [
    "# 3.With LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31960427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297579f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash_1 = GoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY_1\"))\n",
    "gemini_flash_2 = GoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY_2\"))\n",
    "gemini_flash_3 = GoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY_3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysprompt = \"\"\"\n",
    "# La définition d’autosuffisance\n",
    "\n",
    "L'autosuffisance est la démarche visant à acquérir la capacité de subvenir par soi-même à ses besoins fondamentaux, \n",
    "en premier lieu alimentaires par l'autoconsommation – c'est-à-dire produire, récolter et conserver un maximum de sa propre nourriture, \n",
    "souvent en privilégiant le bio, le local et le saisonnier.\n",
    "\n",
    "L'autosuffisance va au-delà de la simple autonomie matérielle : elle représente un engagement volontaire pour être moins dépendant du système économique et social extérieur.  \n",
    "Cela implique des choix concrets comme trouver un lieu propice, le concevoir judicieusement (par exemple en permaculture), changer sa manière de valoriser son temps et de consommer, et former ainsi le fondement d'une vie plus autonome.\n",
    "\n",
    "---\n",
    "\n",
    "# Votre mission\n",
    "\n",
    "- Décider si une chaîne concerne la thématique de l’autosuffisance en se basant sur un contexte qui est soit :\n",
    "   - **context** = nom de la chaîne + bio\n",
    "   - OU  \n",
    "   - **context** = nom de la chaîne + bio + titres, descriptions, tags des trois dernières vidéos (pour les chaînes dont le bio est très court ou vide)\n",
    "\n",
    "- **Répondre \"oui\" uniquement pour les chaînes qui abordent directement l’autosuffisance et l’autonomie.**\n",
    "   Cela inclut :\n",
    "    Chaînes personnelles montrant leur démarche d’autosuffisance (ex. potager, élevage, énergie, conservation, autonomie alimentaire).  \n",
    "    Chaînes médias explicitement centrées sur l’autosuffisance (interviews, conseils, reportages dédiés à l’autosuffisance).\n",
    "    Chaînes qui abordent des concepts très liés à l’autosuffisance\n",
    "\n",
    "- La présence de mots-clés comme autosuffisance, autosuffisant, autonomie, vie autonome, autonomie alimentaire, autonomie énergétique est un bon indicateur de la pertinence de la chaîne.\n",
    "\n",
    "---\n",
    "\n",
    "# Format attendu\n",
    "\n",
    "La réponse doit être au format JSON :\n",
    "\n",
    "  \"decision\": \"oui\" ou \"non\",\n",
    "  \"justification\": \"Justifiez votre décision avec des arguments clairs, expliquant pourquoi la chaîne est ou n’est pas directement liée à l’autosuffisance selon les critères ci-dessus.\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "userprompt = \"\"\"\n",
    "Le context ici\n",
    "---\n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", sysprompt),\n",
    "    (\"user\", userprompt)\n",
    "])\n",
    "\n",
    "chain_1 = prompt | gemini_flash_1\n",
    "chain_2 = prompt | gemini_flash_2\n",
    "chain_3 = prompt | gemini_flash_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d570f8f9",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7c1e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': (\"\\n                              Nom Chaine :\\n                              ---\\n                              L’Ôton’home\\n                              \\n                              Bio Chaine :\\n                              ---\\n                              Nous sommes un couple parent de 2 jeunes garçons et avons acheté à l’été 2020 une GRANDE GRANGE sur un peu plus de 3 hectares de terrain dans le Tarn. Notre objectif: transformer cette grange en HABITATION la plus AUTONOME possible, ce par nos propres moyens, et en faire notre petit paradis. Venez partager notre aventure et suivre ce projet un peu fou ici.\\n\\nLe but de notre chaîne: montrer toutes les étapes de la TRANSFORMATION de la GRANGE en HABITATION, échanger sur l’AUTONOMIE, partager nos DIY en matière de PRODUITS MÉNAGERS et COSMÉTIQUES, toujours dans une optique d’AUTONOMIE.\\n\\nPrécision importante: nous ne sommes PAS des professionnels du bâtiment donc nos vidéos n'ont pas valeur de tutoriels mais de simples partages d'expérience.\\n\\nN’hésitez pas à laisser vos commentaires et à vous abonner à la chaîne. Vous pouvez également nous retrouver sur Facebook et Tipeee. En espérant pouvoir vous inspirer.\\n\\nDes bises,\\n\\nAurélie & David \\n                            \",)}\n",
      "```json\n",
      "{\n",
      "  \"decision\": \"oui\",\n",
      "  \"justification\": \"La chaîne \\\"L’Ôton’home\\\" aborde directement l’autosuffisance et l’autonomie. Le bio de la chaîne mentionne explicitement l'objectif de transformer une grange en habitation la plus autonome possible, de partager des DIY en matière de produits ménagers et cosmétiques dans une optique d'autonomie. Les mots-clés \\\"autonomie\\\" et \\\"habitation autonome\\\" sont présents.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "llmInput = \"\\n                              Nom Chaine :\\n                              ---\\n                              L’Ôton’home\\n                              \\n                              Bio Chaine :\\n                              ---\\n                              Nous sommes un couple parent de 2 jeunes garçons et avons acheté à l’été 2020 une GRANDE GRANGE sur un peu plus de 3 hectares de terrain dans le Tarn. Notre objectif: transformer cette grange en HABITATION la plus AUTONOME possible, ce par nos propres moyens, et en faire notre petit paradis. Venez partager notre aventure et suivre ce projet un peu fou ici.\\n\\nLe but de notre chaîne: montrer toutes les étapes de la TRANSFORMATION de la GRANGE en HABITATION, échanger sur l’AUTONOMIE, partager nos DIY en matière de PRODUITS MÉNAGERS et COSMÉTIQUES, toujours dans une optique d’AUTONOMIE.\\n\\nPrécision importante: nous ne sommes PAS des professionnels du bâtiment donc nos vidéos n'ont pas valeur de tutoriels mais de simples partages d'expérience.\\n\\nN’hésitez pas à laisser vos commentaires et à vous abonner à la chaîne. Vous pouvez également nous retrouver sur Facebook et Tipeee. En espérant pouvoir vous inspirer.\\n\\nDes bises,\\n\\nAurélie & David \\n                            \",\n",
    "input = {\"context\":llmInput}\n",
    "print(input)\n",
    "print(chain_1.invoke(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "77e4c83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"decision\": \"oui\",\n",
      "  \"justification\": \"La chaîne mentionne explicitement la permaculture et les solutions écologiques durables, qui sont des concepts liés à l'autosuffisance. De plus, elle évoque un mode de vie plus simple et durable, ce qui suggère une démarche d'autonomie.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "llmInput = \"\\n                              Nom Chaine :\\n                              ---\\n                              Réenchantons la Terre\\n                              \\n                              Bio Chaine :\\n                              ---\\n                              🐾🌍🎶 Réenchantons la Terre :\\n🌻 Explorer, connecter, célébrer à travers des vidéos sur :\\n- La vie en écovillage et la permaculture.\\n- La communication non violente et les pédagogies actives.\\n- Les pratiques somatiques et les solutions écologiques durables.\\n✨ Partager des paroles positives et engagées pour inspirer l’action.\\n🌈 Oser franchir le pas vers un mode de vie plus simple, durable et joyeux.\\n\\n-----\\n🐾🌍🎶 Re-Enchanting the Earth:\\n🌻 Explore, connect, and celebrate through videos about:\\n- Life in ecovillages and permaculture.\\n- Nonviolent communication and active pedagogies.\\n- Somatic practices and sustainable ecological solutions.\\n✨ Sharing positive and committed messages to inspire action.\\n🌈 Daring to take the step towards a simpler, sustainable, and joyful way of living.\\n \\n                            \"\n",
    "input = {\"context\":llmInput}\n",
    "print(chain_1.invoke(input)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14257a7d",
   "metadata": {},
   "source": [
    "## Run on the samples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a1897",
   "metadata": {},
   "source": [
    "### prepare the LLM input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d5d29b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1836"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echantillon = openJson(\"../jsons/echantillon.json\")\n",
    "channelsF2  = openJson(\"../jsons/channelsF2.json\")\n",
    "len(channelsF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b79ca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      " json saved\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "echantillonPredictions= []\n",
    "\n",
    "for item in echantillon:\n",
    "    llmInput = \"\"\n",
    "    for channel in channelsF2:\n",
    "        if  item['id_chaine']==channel['id_chaine']:\n",
    "            if \"context\" in channel:\n",
    "              llmInput = f\"\"\"\n",
    "                              Nom Chaine :\n",
    "                              ---\n",
    "                              {channel['nom_chaine']}\n",
    "                              \n",
    "                              Bio Chaine :\n",
    "                              ---\n",
    "                              {channel['bio']}\n",
    "                              \n",
    "                              contexte d’après les vidéos :\n",
    "                              ---\n",
    "                              {channel['context']}   \n",
    "                          \"\"\"          \n",
    "            else :\n",
    "                 llmInput = f\"\"\"\n",
    "                              Nom Chaine :\n",
    "                              ---\n",
    "                              {channel['nom_chaine']}\n",
    "                              \n",
    "                              Bio Chaine :\n",
    "                              ---\n",
    "                              {channel['bio']} \n",
    "                            \"\"\"\n",
    "            break\n",
    "    echantillonPredictions.append({\n",
    "      'id_chaine':item['id_chaine'],\n",
    "      'llmInput':llmInput\n",
    "    })      \n",
    "\n",
    "saveJson(\"../jsons/echantillonPredictions.json\",echantillonPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f39cc",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f3b3087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echantillonPredictions = openJson(\"../jsons/echantillonPredictions.json\")\n",
    "len(echantillonPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "14b5460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAnswer(answer):\n",
    "    answer = answer.strip(\"`\")   \n",
    "    if answer.startswith(\"json\"):\n",
    "        answer = answer[4:].strip() \n",
    "    return  answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 39/200 [00:43<02:41,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep for 50 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 79/200 [02:18<02:04,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep for 50 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 95/200 [03:27<02:15,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      " json saved\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 119/200 [03:52<01:20,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep for 50 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 159/200 [05:26<00:44,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep for 50 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 190/200 [06:53<00:11,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      " json saved\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 199/200 [07:03<00:01,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep for 50 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:54<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      " json saved\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count1 = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "temp = 0\n",
    "\n",
    "for item in tqdm(echantillonPredictions):\n",
    "  \n",
    "    input = {\"context\":item[\"llmInput\"]}\n",
    "    try:\n",
    "        if count1  <= 13:\n",
    "            #print(\"KEY 1\")\n",
    "        \n",
    "            answer = cleanAnswer(chain_1.invoke(input))\n",
    "            answer = json.loads(answer)\n",
    "            #print(video['id_video'],'/n',answer)\n",
    "            item.update(answer)\n",
    "            \n",
    "            count1+=1\n",
    "            temp+=1            \n",
    "            \n",
    "            #print(\"count1 \",count1)  \n",
    "            \n",
    "        if count1  > 13 and count2 <= 13:\n",
    "            #print(\"KEY 2\")\n",
    "            \n",
    "            answer = cleanAnswer(chain_2.invoke(input))\n",
    "            answer = json.loads(answer)\n",
    "            #print(video['id_video'],'/n',answer)\n",
    "            item.update(answer)\n",
    "            \n",
    "            count2+=1\n",
    "            temp+=1\n",
    "            #print(\"count2 \",count2) \n",
    "            \n",
    "        if  count1  > 13 and count2  > 13 and count3 <= 13:\n",
    "            \n",
    "            #print(\"KEY 3\")\n",
    "            \n",
    "            answer = cleanAnswer(chain_3.invoke(input))\n",
    "            answer = json.loads(answer)\n",
    "            #print(video['id_video'],'/n',answer)\n",
    "            item.update(answer)\n",
    "            \n",
    "            count3+=1\n",
    "            temp+=1\n",
    "            #print(\"count3 \",count3) \n",
    "                \n",
    "        if count1 > 13 and count2 > 13 and count3 > 13 :\n",
    "            print(\"sleep for 50 s\")\n",
    "            time.sleep(50)\n",
    "            count1 = 0\n",
    "            count2 = 0\n",
    "            count3 = 0\n",
    "        \n",
    "\n",
    "        if temp >= 100:\n",
    "            saveJson(\"../jsons/echantillonPredictions.json\",echantillonPredictions)\n",
    "            temp=0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"error {e}\")\n",
    "        \n",
    "saveJson(\"../jsons/echantillonPredictions.json\",echantillonPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aadbda",
   "metadata": {},
   "source": [
    "## Validate the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2325b958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundTruth = openJson(\"../jsons/echantillon.json\")\n",
    "predictions = openJson(\"../jsons/echantillonPredictions.json\")\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7339e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvaluationMetrics(TP,TN,FP,FN):\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    #print(f\"Accuracy : {accuracy:.2%}\")\n",
    "    #print(f\"F1-score : {f1_score:.2%}\")\n",
    "    return round(accuracy,2),round(f1_score,2)\n",
    "\n",
    "def getGround(channelID):\n",
    "    for channel in groundTruth:\n",
    "        if channel['id_chaine']==channelID:\n",
    "            return channel['pertinente']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c2d30a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate():\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN = 0\n",
    "    for pred in predictions:\n",
    "        y_reel = getGround(pred['id_chaine'])\n",
    "        y_pred = pred['decision']\n",
    "        if y_pred == 'oui':\n",
    "            if y_reel=='oui':\n",
    "                TP+=1\n",
    "            else :\n",
    "                FP+=1\n",
    "                #print(pred['id_chaine'])\n",
    "        else:\n",
    "            if y_reel=='non':\n",
    "                TN+=1\n",
    "            else :\n",
    "                #print(pred['id_chaine'])\n",
    "                FN+=1\n",
    "    print(f\"Total FP: {FP}\")\n",
    "    print(f\"Total FN: {FN}\")\n",
    "    print(f\"Total TP: {TP}\")\n",
    "    print(f\"Total TN: {TN}\")    \n",
    "    return getEvaluationMetrics(TP,TN,FP,FN)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FP: 8\n",
      "Total FN: 13\n",
      "Total TP: 21\n",
      "Total TN: 158\n",
      "accuarcy : 0.9\n",
      "f1_score : 0.67\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1_score = validate()\n",
    "print(f\"accuarcy : {accuracy}\")\n",
    "print(f\"f1_score : {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e1e7a",
   "metadata": {},
   "source": [
    "# Apply on all Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eac3c1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1836"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channelsF2 = openJson(\"../jsons/channelsF2.json\")\n",
    "len(channelsF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf0759",
   "metadata": {},
   "source": [
    "## prepare llmInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd794dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      " json saved\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for channel in channelsF2:\n",
    "    llmInput = \"\"\n",
    "    if \"context\" in channel:\n",
    "        llmInput = f\"\"\"\n",
    "                    Nom Chaine :\n",
    "                    ---\n",
    "                    {channel['nom_chaine']}\n",
    "                    \n",
    "                    Bio Chaine :\n",
    "                    ---\n",
    "                    {channel['bio']}\n",
    "                    \n",
    "                    contexte d’après les vidéos :\n",
    "                    ---\n",
    "                    {channel['context']}   \n",
    "                \"\"\"          \n",
    "    else :\n",
    "        llmInput = f\"\"\"\n",
    "                    Nom Chaine :\n",
    "                    ---\n",
    "                    {channel['nom_chaine']}\n",
    "                    \n",
    "                    Bio Chaine :\n",
    "                    ---\n",
    "                    {channel['bio']} \n",
    "                \"\"\"\n",
    "    channel['llmInput'] = llmInput   \n",
    "    \n",
    "saveJson(\"../jsons/channelsF2.json\",channelsF2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3bc99",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11874943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAnswer(answer):\n",
    "    answer = answer.strip(\"`\")   \n",
    "    if answer.startswith(\"json\"):\n",
    "        answer = answer[4:].strip() \n",
    "    return  answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba96dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "temp = 0\n",
    "\n",
    "for item in tqdm(channelsF2):\n",
    "  \n",
    "    input = {\"context\":item[\"llmInput\"]}\n",
    "    try:\n",
    "        if count1  <= 13:\n",
    "            #print(\"KEY 1\")\n",
    "        \n",
    "            answer = cleanAnswer(chain_1.invoke(input))\n",
    "            answer = json.loads(answer)\n",
    "            #print(video['id_video'],'/n',answer)\n",
    "            item.update(answer)\n",
    "            \n",
    "            count1+=1\n",
    "            temp+=1            \n",
    "            \n",
    "            #print(\"count1 \",count1)  \n",
    "            \n",
    "        if count1  > 13 and count2 <= 13:\n",
    "            #print(\"KEY 2\")\n",
    "            \n",
    "            answer = cleanAnswer(chain_2.invoke(input))\n",
    "            answer = json.loads(answer)\n",
    "            #print(video['id_video'],'/n',answer)\n",
    "            item.update(answer)\n",
    "            \n",
    "            count2+=1\n",
    "            temp+=1\n",
    "            #print(\"count2 \",count2) \n",
    "            \n",
    "        if  count1  > 13 and count2  > 13 and count3 <= 13:\n",
    "            \n",
    "            #print(\"KEY 3\")\n",
    "            \n",
    "            answer = cleanAnswer(chain_3.invoke(input))\n",
    "            answer = json.loads(answer)\n",
    "            #print(video['id_video'],'/n',answer)\n",
    "            item.update(answer)\n",
    "            \n",
    "            count3+=1\n",
    "            temp+=1\n",
    "            #print(\"count3 \",count3) \n",
    "                \n",
    "        if count1 > 13 and count2 > 13 and count3 > 13 :\n",
    "            print(\"sleep for 50 s\")\n",
    "            time.sleep(50)\n",
    "            count1 = 0\n",
    "            count2 = 0\n",
    "            count3 = 0\n",
    "        \n",
    "\n",
    "        if temp >= 100:\n",
    "            saveJson(\"../jsons/channelsF2.json\",echantillonPredictions)\n",
    "            temp=0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"error {e}\")\n",
    "        \n",
    "saveJson(\"../jsons/channelsF2.json\",echantillonPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8628121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
