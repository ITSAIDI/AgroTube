{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from collections import Counter\n",
    "from tqdm import tqdm \n",
    "from colorama import Style,Fore\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d64c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils1 import openJson,saveJson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1883a",
   "metadata": {},
   "source": [
    "## Filtering 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9957c",
   "metadata": {},
   "source": [
    "- videosF1.json contain the videos with fr-FR on the language parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosR3 = openJson(\"../jsons/videosR3.json\")\n",
    "len(videosR3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70157702",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosF1 = []\n",
    "for video in videosR3:\n",
    "    if video['langue']== 'fr-FR':\n",
    "        videosF1.append(video)\n",
    "saveJson(\"../jsons/videosF1.json\",videosF1)\n",
    "len(videosF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosF1 = openJson(\"../jsons/videosF1.json\")\n",
    "len(videosF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3116da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ace6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc12b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d0c5eb6",
   "metadata": {},
   "source": [
    "## Filtering 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9308cab",
   "metadata": {},
   "source": [
    "### Detecter les videos des Chaines TV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chainesTv = [\n",
    "    \"France 2\", \"France 3\", \"France 4\",\"France 5\",\"Franceinfo\",\n",
    "    \"BFMTV\", \"C8\", \"CStar\", \"Gulli\", \"Cnews\",\n",
    "    \"Canal+\", \"Planète+\", \"LCI\", \"Paris première\",\n",
    "    \"6ter\", \"Arte\", \"M6\", \"W9\",\n",
    "    \"TFX\", \"TMC\", \"NRJ12\", \"TF1\",\"La Chaîne parlementaire\",\n",
    "    \"Chérie 25\", \"RMC\"\n",
    "]\n",
    "chainesTv = [nomTV.lower().replace(\" \", \"\") for nomTV in chainesTv]\n",
    "\n",
    "print(len(chainesTv))\n",
    "print(chainesTv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosF1 = openJson(\"../jsons/videosF1.json\")\n",
    "len(videosF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = openJson(\"../../collecting/jsons/channels.json\")\n",
    "def getChannelName(channelId):\n",
    "    for channel in channels:\n",
    "        if channel['id_chaine']==channelId:\n",
    "            return channel['nom_chaine'].lower().replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "chainesTVCounter = Counter()\n",
    "videosChainesTV = []\n",
    "for video in videosF1:\n",
    "    channelName = getChannelName(video['id_chaine'])\n",
    "    for name in chainesTv:\n",
    "        if name in channelName:  \n",
    "            print(name,' ',channelName,' ',video['id_video'])\n",
    "            chainesTVCounter[name]+=1\n",
    "            videosChainesTV.append(video['id_video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chainesTVCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = list(chainesTVCounter.keys())\n",
    "sizes = list(chainesTVCounter.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(labels, sizes, color='skyblue')\n",
    "\n",
    "plt.title(\"Répartition des vidéos des chaînes TV\", fontsize=14)\n",
    "plt.xlabel(\"Chaînes TV\", fontsize=12)\n",
    "plt.ylabel(\"Nombre de vidéos\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "total_videos_tv = sum(sizes)\n",
    "plt.text(len(labels) - 2, max(sizes) * 0.8, f\"Total : {total_videos_tv} vidéos\", fontsize=12, ha='center')\n",
    "plt.text(len(labels) - 2, max(sizes) * 0.74, \"4.62% d'ensemble des vidéos\", fontsize=12, ha='center')\n",
    "\n",
    "# Ajouter les nombres au-dessus des barres et les rendre en gras\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, f'{int(yval)}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e073204",
   "metadata": {},
   "outputs": [],
   "source": [
    "(218/len(videosF1))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545f6db",
   "metadata": {},
   "source": [
    "### Creer videosF2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ccc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosChainesTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a140c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(videosChainesTV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosF2 = []\n",
    "for video in videosF1:\n",
    "    if video['id_video'] not in videosChainesTV:\n",
    "        videosF2.append(video)\n",
    "saveJson(\"../jsons/videosF2.json\",videosF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f067c2",
   "metadata": {},
   "source": [
    "verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d7bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(videosF1)-len(videosF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a279aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(videosF1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c175f",
   "metadata": {},
   "source": [
    "## Filtering 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f44f5",
   "metadata": {},
   "source": [
    "### With Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cbae660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"dangvantuan/sentence-camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f90b6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "def calculateSimilarity(videoMD, documents):\n",
    "    embedding_video = model.encode([videoMD], convert_to_tensor=True, normalize_embeddings=True)\n",
    "    embeddings_documents = model.encode(documents, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    scores = util.cos_sim(embedding_video, embeddings_documents)[0]\n",
    "    simScore = scores.mean().item()\n",
    "    return simScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02986be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary = openJson(\"../jsons/Voc.json\")\n",
    "len(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "832b19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construire_phrases(mots_cles, taille=25):\n",
    "    phrases = []\n",
    "    for i in range(0, len(mots_cles), taille):\n",
    "        groupe = mots_cles[i:i+taille]\n",
    "        phrase = ' '.join(groupe)\n",
    "        phrases.append(phrase)\n",
    "    return phrases\n",
    "\n",
    "documents = construire_phrases(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6fc110",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "print(documents[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b542f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "requete = \"Mes livres préférés pour l'AUTONOMIE/AUTOSUFFISANCE : potager, élevage, cuisine, transfo ect)\"\n",
    "\n",
    "calculateSimilarity(requete,documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ff30898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1091/4504 [18:13<54:50,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4031078815460205    ybRHmx3ZnEs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1092/4504 [18:14<54:45,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29167455434799194    sXOUONI5as8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1093/4504 [18:15<53:50,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21356217563152313    zFa0rPdBUqM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1094/4504 [18:16<52:35,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22306613624095917    UNfsFwj_aFA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1095/4504 [18:17<52:43,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2536340355873108    ovldBmlaE10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1096/4504 [18:18<52:11,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44524794816970825    Wq3JCclf1FE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1097/4504 [18:19<53:10,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583921432495117    FkBrcHP9ZZE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1098/4504 [18:20<51:47,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3969900608062744    2dGr7WJBj0o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1099/4504 [18:20<50:07,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45982393622398376    ivqfNFAfJCU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1100/4504 [18:21<48:39,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4082881808280945    W5xbFMt3t4Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1101/4504 [18:22<50:53,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35788577795028687    Kceo8Q6WbBg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1102/4504 [18:23<51:29,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39416396617889404    __5cfHpaXfU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1102/4504 [18:24<56:50,  1.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m tqdm(videosF2):\n\u001b[1;32m      4\u001b[0m     context \u001b[38;5;241m=\u001b[39m video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitre_video\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculateSimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(score,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m,video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_video\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m      8\u001b[0m         {\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_video\u001b[39m\u001b[38;5;124m'\u001b[39m:video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_video\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m:score\n\u001b[1;32m     11\u001b[0m         }\n\u001b[1;32m     12\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m, in \u001b[0;36mcalculateSimilarity\u001b[0;34m(videoMD, documents)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalculateSimilarity\u001b[39m(videoMD, documents):\n\u001b[1;32m      4\u001b[0m     embedding_video \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode([videoMD], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m     embeddings_documents \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(embedding_video, embeddings_documents)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m     simScore \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:685\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 685\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    687\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:758\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    757\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     key: value\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    444\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:1037\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1037\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1049\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1050\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:649\u001b[0m, in \u001b[0;36mCamembertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    638\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    639\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    640\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         output_attentions,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:580\u001b[0m, in \u001b[0;36mCamembertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    577\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    578\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 580\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:253\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:592\u001b[0m, in \u001b[0;36mCamembertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 592\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:490\u001b[0m, in \u001b[0;36mCamembertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 490\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AgroTube/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "videosF2 = openJson(\"../jsons/videosF2.json\")\n",
    "scores = []\n",
    "for video in tqdm(videosF2):\n",
    "    context = video['titre_video'] + video['description'] + ' '.join(video['tags'])\n",
    "    score = calculateSimilarity(context,documents)\n",
    "    print(score,'  ',video['id_video'])\n",
    "    scores.append(\n",
    "        {\n",
    "            'id_video':video['id_video'],\n",
    "            'similarity':score\n",
    "        }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      " json saved\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "saveJson(\"../jsons/keywordsVsMTD.json\",scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "videosNon = openJson(\"../jsons/llmVideosNon.json\")\n",
    "keywordsVsMTD = openJson(\"../jsons/keywordsVsMTD.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3045061230659485\n",
      "0.08022268116474152\n",
      "0.2510215640068054\n",
      "0.13914254307746887\n",
      "0.3456891179084778\n",
      "0.35787028074264526\n",
      "0.1502985656261444\n",
      "0.16819684207439423\n",
      "0.1344427466392517\n",
      "0.2256837785243988\n",
      "0.23916630446910858\n",
      "0.2077063024044037\n",
      "0.20742538571357727\n",
      "0.14259026944637299\n",
      "0.23402783274650574\n",
      "0.15943597257137299\n",
      "0.12071476131677628\n",
      "0.1028195172548294\n",
      "0.06876079738140106\n",
      "0.18382926285266876\n",
      "0.1969488561153412\n",
      "0.2574109435081482\n",
      "0.4151611924171448\n",
      "0.10599108785390854\n",
      "0.13396696746349335\n",
      "0.11021538823843002\n",
      "0.17402228713035583\n",
      "0.24198539555072784\n",
      "0.31872421503067017\n",
      "0.22191984951496124\n",
      "0.1624375879764557\n",
      "0.025443408638238907\n",
      "0.18030989170074463\n",
      "0.16425389051437378\n",
      "0.19237956404685974\n",
      "0.22844040393829346\n",
      "0.20870497822761536\n",
      "0.2546618580818176\n",
      "0.07434268295764923\n",
      "0.023086562752723694\n",
      "0.359218031167984\n",
      "0.1608438789844513\n",
      "0.34426793456077576\n",
      "0.16758760809898376\n",
      "0.11607363075017929\n"
     ]
    }
   ],
   "source": [
    "Sum = []\n",
    "for video in videosNon:\n",
    "    for result in keywordsVsMTD:\n",
    "        if result['id_video']==video['id_video']:\n",
    "            print(result['similarity'])\n",
    "            Sum.append(result['similarity'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8c60280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19248774987128045\n"
     ]
    }
   ],
   "source": [
    "nn = 0\n",
    "for fuck in Sum:\n",
    "    nn+=fuck\n",
    "print(nn/len(Sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1235db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexte_autosuffisance = [\n",
    "    \"Potager familial productif pour l'autonomie en légumes\",\n",
    "    \"Élevage de poules, moutons et canards pour l’autonomie alimentaire\",\n",
    "    \"Utilisation de plantes médicinales et aromatiques cultivées sur place\",\n",
    "    \"Récupération et filtration de l'eau via un puits et osmose inverse\",\n",
    "    \"Construction de bacs permacoles faits maison pour une culture efficace\",\n",
    "    \"Transformation du sol en terre fertile pour maximiser les rendements\",\n",
    "    \"Vie en sobriété heureuse et respect de la nature\",\n",
    "    \"Utilisation de semences locales et choix variétal réfléchi\",\n",
    "    \"Production locale de viande, œufs et légumes\",\n",
    "    \"Utilisation d'une serre pour prolonger les cultures\",\n",
    "    \"Métiers artisanaux (ébénisterie, photographie culinaire) intégrés dans le mode de vie\",\n",
    "    \"Conception d’un lieu de vie en autonomie énergétique et alimentaire\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1027eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosF2 = openJson(\"../jsons/videosF2.json\")\n",
    "scores = {}\n",
    "for video in tqdm(videosF2):\n",
    "    context = video['titre_video'] + video['description'] + ' '.join(video['tags'])\n",
    "    score = calculateSimilarity(context,documents)\n",
    "    print(score,'  ',video['id_video'])\n",
    "    scores[video['id_video']]=score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8579331",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosF2 = openJson(\"../jsons/videosF2.json\")\n",
    "scores = {}\n",
    "for video in tqdm(videosF2):\n",
    "    context = video['titre_video'] + video['description'] + ' '.join(video['tags'])\n",
    "    score = calculateSimilarity(context,contexte_autosuffisance)\n",
    "    print(score,'  ',video['id_video'])\n",
    "    scores[video['id_video']]=score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d311a9",
   "metadata": {},
   "source": [
    "### LLM Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash_1 = GoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY_1\"))\n",
    "gemini_flash_2 = GoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e48db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysprompt = \"\"\"\n",
    "# La definition d'autossuffinnce\n",
    "\n",
    "L'autosuffisance est la démarche visant à acquérir la capacité de subvenir par soi-même à ses besoins fondamentaux, \n",
    "en premier lieu alimentaires par l'autoconsommation – c'est-à-dire produire, récolter et conserver un maximum de sa propre nourriture, \n",
    "souvent en privilégiant le bio, le local et le saisonnier. Plus qu'une simple recherche d'autonomie matérielle, \n",
    "elle représente un engagement pour être moins dépendant du système économique et social extérieur, impliquant des choix concrets comme trouver un lieu propice et le concevoir judicieusement (par exemple en permaculture), \n",
    "ainsi qu'un changement dans la manière de valoriser son temps et de consommer, formant ainsi le fondement d'une vie plus autonome.\n",
    "\n",
    "---\n",
    "\n",
    "# Votre mission\n",
    "\n",
    "- Décider si une vidéo concerne la thématique de l'autosuffisance en se basant sur ses métadonnées (titre, description, tags).  \n",
    "- Les vlogs, les interviews et les vidéos de partage d'expériences en lien avec l'autosuffisance sont intéressants.  \n",
    "- Les tutoriels et les vidéos de conseils sont également pertinents.  \n",
    "- Les vidéos présentant une technique liée à l'autosuffisance ou y contribuant sont utiles.  \n",
    "- Attribuer un score entier de 1 à 10 pour évaluer la pertinence de la vidéo par rapport à la thématique de l'autosuffisance.\n",
    "\n",
    "---\n",
    "\n",
    "# Votre réponse  \n",
    "\n",
    "- La réponse doit être au format JSON :  \n",
    "  \n",
    "    \"decision\": \"oui ou non\",\n",
    "    \"justification\": \"Justifiez votre décision avec des arguments\"\n",
    "    \"score\": votre èvaluation\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "userprompt = \"\"\"\n",
    "Titre\n",
    "---\n",
    "{titre}\n",
    "Description\n",
    "---\n",
    "{description}\n",
    "tags\n",
    "---\n",
    "{tags}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", sysprompt),\n",
    "    (\"user\", userprompt)\n",
    "])\n",
    "\n",
    "chain_1 = prompt | gemini_flash_1\n",
    "chain_2 = prompt | gemini_flash_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da5f6b",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"titre\":\"Tuteurer et redresser un arbre 🌳\",\n",
    "         \"description\":\"Dans cette vidéo nous allons voir ensemble comment mettre un tuteur afin de redresser un arbre qui penche suite à la tempête.\\nCette vidéo vous aideras pour placer un tuteur en travers , sangler un arbre selon le sens du vent,\\n\\nIl existe plusieurs facon de tuteurer un arbre, haubanage, tripode,.. mais dans le cas présent un tuteur en biais suffiras puisque l'arbre n'est pas trés gros.\\n\\n\\n▬▬▬▬▬▬▬▬▬▬▬ POUR ALLER PLUS LOIN  ▬▬▬▬▬▬▬▬▬▬▬\\n\\n💡 Abonnez-vous pour me soutenir 🧡\\n\\n\\n💡Rejoignez-nous sur twitter\\net tik tok\\n\\n\\n▬▬▬▬▬▬▬▬▬▬▬ CHAPITRES DE CETTE VIDÉO ▬▬▬▬▬▬▬▬▬▬\\n\\n00:00 : Introduction\\n00:23 : Mise en place tuteur\\n01:54 : Sanglage et Redresser l'arbre\\n02:54 : Sens du vent et détails tuteurage\\n\\n▬▬▬▬▬▬▬▬▬▬▬ A PROPOS DE CETTE VIDÉO ▬▬▬▬▬▬▬▬▬▬▬\\n\\nLe Jardin du moulin\\nFrancois\\n#potager #jardindumoulin\",\n",
    "         \"tags\":', '.join( [\n",
    "      \"TCS\",\n",
    "      \"Agriculture Biologique\",\n",
    "      \"Bio\",\n",
    "      \"GAB 85\",\n",
    "      \"FNAB\",\n",
    "      \"Vendée\",\n",
    "      \"Techniques Culturales Simplifiées\",\n",
    "      \"GAEC Les Jonquilles\",\n",
    "      \"ECOPHYTO\",\n",
    "      \"Demain Vendée\",\n",
    "      \"Conservation des sols\",\n",
    "      \"Sol vivant\",\n",
    "      \"Productions végétales\",\n",
    "      \"Maïs\"\n",
    "    ])}\n",
    "print(input)\n",
    "print(chain_2.invoke(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAnswer(answer):\n",
    "    answer = answer.strip(\"`\")   # Supprimer tous les backticks\n",
    "    if answer.startswith(\"json\"):\n",
    "        answer = answer[4:].strip() \n",
    "    return  answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82038bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosF2 = openJson(\"../jsons/videosF2.json\")\n",
    "llmEvaluation = []\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "temp = 0\n",
    "\n",
    "for video in tqdm(videosF2):\n",
    "    input = {\"titre\":video['titre_video'],\n",
    "             \"description\":video['description'],\n",
    "             \"tags\":', '.join(video['tags']),}\n",
    "    try:\n",
    "        if count1  <= 13:\n",
    "            print(\"KEY 1\")\n",
    "        \n",
    "            answer = cleanAnswer(chain_1.invoke(input))\n",
    "            #print(video['id_video'],'/n',answer)\n",
    "            evaluation = {}\n",
    "            evaluation['id_video']=video['id_video']\n",
    "            evaluation.update(json.loads(answer))\n",
    "            llmEvaluation.append(evaluation)\n",
    "            \n",
    "            count1+=1\n",
    "            temp+=1            \n",
    "             \n",
    "            print(\"count1 \",count1)  \n",
    "            \n",
    "        if count1  > 13 and count2 <= 13:\n",
    "            print(\"KEY 2\")\n",
    "            \n",
    "            answer =  cleanAnswer(chain_2.invoke(input))\n",
    "            #print(video['id_video'],'/n',answer)\n",
    "            evaluation = {}\n",
    "            evaluation['id_video']=video['id_video']\n",
    "            evaluation.update(json.loads(answer))\n",
    "            llmEvaluation.append(evaluation)\n",
    "            \n",
    "            count2+=1\n",
    "            temp+=1\n",
    "            print(\"count2 \",count2)  \n",
    "            \n",
    "        if count1 > 13 and count2 > 13 :\n",
    "            print(\"sleep for 1 min\")\n",
    "            time.sleep(60)\n",
    "            count1 = 0\n",
    "            count2 = 0\n",
    "        \n",
    "\n",
    "        if temp >= 100:\n",
    "            saveJson(\"../jsons/llmEvaluation.json\",llmEvaluation)\n",
    "            temp=0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"error {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ed73c",
   "metadata": {},
   "source": [
    "- Seperate the videos classified as not relevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e48bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmEvaluations = openJson(\"../jsons/llmEvaluation.json\")\n",
    "videosF2 = openJson(\"../jsons/videosF2.json\")\n",
    "\n",
    "def getNonVideo(ID):\n",
    "    for video in videosF2:\n",
    "        if video['id_video']==ID:\n",
    "            return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(videosF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llmEvaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosNon = []\n",
    "for evaluation in llmEvaluations:\n",
    "    try:\n",
    "\n",
    "        if evaluation['decision'] =='non':\n",
    "            videoNo = getNonVideo(evaluation['id_video'])\n",
    "            videoNo.update(evaluation)\n",
    "            videosNon.append(videoNo)\n",
    "    except:\n",
    "        print(evaluation)\n",
    "        break\n",
    "    \n",
    "saveJson(\"../jsons/llmVideosNon.json\",videosNon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "videosNon = openJson(\"../jsons/llmVideosNon.json\")\n",
    "len(videosNon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66809b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38b2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
